
English Readme | [[README-CN.org][中文说明]]

** MariaDB/MySQL to Elasticsearch

   Synchronize the MariaDB/MySQL table data to Elasticsearch, only supports adding and updating,
   does not support physical deletion (physical deletion needs to be processed according to binlog),
   it is recommended to use logical deletion. For example, add the deleted(1 deleted, default 0) field to the table.

   Based on jdk 8 and spring boot 2, support Elasticsearch 6.3.2


The relevant configuration instructions are as follows:
#+BEGIN_SRC yaml
spring:
  datasource:
    ... for db ...
  elasticsearch.rest:
    uris: [192.168.1.2:9200,192.168.1.3:9200]  # The default is 127.0.0.1:9200

db2es:
  cron: 0/5 * * * * *            # The default is to execute once per minute

  relation:
    -
      table: t_order           # *** Must set and have primary key. The primary key will generate the id of /index/type/id in Elasticsearch, if has multi, id where append with "-". can use % as a wildcard to match multiple tables(when sharding table)
      increment-column: id       # *** Must set. Indicates that it is used for data increment operations. Generally, it uses auto increment ~id~ or ~time~

      # Starting with Elasticsearch 6.0, type defaults to _doc, and the index in Elasticsearch directly corresponds to the database table name
      index: order               # Indicates the index of /index/type/id in Elasticsearch, not set will be generated from the database table name (t_some_one ==> some-one), 6.0 start index name must be lowercase
      type: doc                  # The default is _doc
      scheme: true               # Whether to generate Elasticsearch's scheme based on the database table structure at startup, the default is false
      sql: select * from t_order # Custom sql statement (do not use ORDER BY and LIMIT, will be automatically added based on increment-column), no setting will automatically assemble from the database table
      table-alias: o             # If sql has join multi table, main table's alias
      limit: 2000                # The number of times to get from the database, the default is 1000

      route-column: [c1,c2]      # The fields used for routing, multiple separated by commas
      pattern-to-id: false       # true: the wildcard data of the table name is used as part of the id(for example, table use t_order_% wildcard, then the table t_order_2016 will be used 2016 to the prefix of the id), the default is true
      key-column: [c1,c2]        # The id column used to generate the index will not be automatically retrieved from the table. When the table has a primary key and multiple columns of unique index, can use this configuration when you want to use the unique index to do the index id.
      id-prefix:                 # Use when you want to prefix the index id
      id-suffix:                 # Use when you want to suffix the index id

      mapping:                   # By default, it will be generated from the table field (c_some_type ==> someType), and only special cases can set.
        c_type: type               # table column(Use alias if there is an alias) : elasticsearch field
      ignore-column: [c1,c2]     # The above sql does not want to write the index of the column (if the column has an alias, use the alias)

      big-count-to-sql: 2000     # Limit start in sql, start in 1000 exceeds this value will be optimized into inner join statement, the default is 2000
      primary-key: id            # The primary key name, when the table data is a lot, use  LIMIT 10million,1000  efficiency will be very slow, this field will optimize the sql statement, the default is id
      # Original sql :  SELECT ... FROM t_table WHERE time > '2010-01-01 00:00:01' LIMIT 10million,1000
      # Optimized sql:  SELECT ... FROM t_table c inner join (SELECT id FROM t_table WHERE time > '2010-01-01 00:00:01' LIMIT 10million,1000) t on t.id = c.id

      nested-mapping:
        # If has table `t_order` and `t_order_item`, `t_order` : id, `t_order_item` : order_id, then main-field => id, nested-field => order_id
        item:
          main-field: id
          sql: SELECT xx, yy, zz FROM order_item
          nested-field: order_id
        address:
          main-field: id
          sql: SELECT aa, bb, cc FROM order_address
          nested-field: order_id
#+END_SRC

about cron
#+BEGIN_EXAMPLE
.------------------- second (0 - 59)   if (0/10) then (0, 10, 20, 30, 40, 50) run
.  .---------------- minute (0 - 59)
.  .  .------------- hour (0 - 23)
.  .  .  .---------- day of month (1 - 31)
.  .  .  .  .------- month (1 - 12)   OR jan,feb,mar,apr,may,jun,jul,aug,sep,oct,nov,dec
.  .  .  .  .  .---- day of week (0 - 6) (Sunday=0 or 7) OR sun,mon,tue,wed,thu,fri,sat
.  .  .  .  .  .
?  *  *  *  *  *

for example:  0/5 * * * * *  means that it runs every 5 seconds
#+END_EXAMPLE


** Run
#+BEGIN_SRC bash
git clone https://github.com/liuanxin/mysql2es.git
cd mysql2es
mvn clean package -DskipTests

# change application-prod.yml to your setting
nohup java -jar -Dspring.profiles.active=prod target/mysql2es.jar >/dev/null 2>&1 &

or

# add your ~/application.yml
nohup java -jar -Dspring.config.location=~/application.yml target/mysql2es.jar >/dev/null 2>&1 &


# log in ~/logs/mysql2es.log
#+END_SRC


** Comment

It is recommended to build the index scheme in Elasticsearch first (if want to generate based on the database table field type, you can set the ~scheme~ to ~true~ in the configuration).  \\

Then, the data is synchronized based on the timing rule.
When synchronizing, the sql splicing increment field is used to obtain the paging data and write Elasticsearch in batches until there is no data.
The value corresponding to the last increment field will be stored in the temporary file,
which will be used in the next synchronization(If it is the next run time but the last time it has not run, it will be postponed).

PS:  \\
If the amount of data is large, the first synchronization takes some time.
Increasing the value of *limit* can increase the synchronization speed.
Temporary files will be generated for each index, such as two indexes with ~product~ and ~order~,
which will generate two temporary files ~/tmp/product~ and ~/tmp/order~.
Windows in the ~C:\Users\current_user\AppData\Local\Temp~ directory,
you can modify it by running ~-Djava.io.tmpdir=/path/to/tmpdir~ on the command line,
If the temporary file corresponding to the index is manually deleted,
the index will be fully manipulated the next time it is synchronized.
