
[[README.org][English Readme]] | 中文说明

** MariaDB/MySQL to Elasticsearch

  同步 MariaDB/MySQL 表的数据到 Elasticsearch, 支持添加和更新, 不支持物理删除(物理删除需要根据 binlog 才能处理),
  建议使用逻辑删除(业务系统使用逻辑删除本身就是一件很自然的事), 比如在表中添加 deleted(1 已删除, 默认是 0)字段.

  基于 jdk 8 和 spring boot 2.0, 支持 Elasticsearch 6.3.2

相关的配置如下:
#+BEGIN_SRC yaml
spring:
  datasource:
    ... for db ...
  elasticsearch.rest:
    uris: [192.168.1.2:9200,192.168.1.3:9200]  # 可以不设定, 默认是 127.0.0.1:9200

config:
  cron: 0/5 * * * * *            # 可以不设定, 默认是每分钟执行一次

  relation:
    -
      table: t_product           # *** 必须设定且要有主键. 主键会生成 Elasticsearch 中 /index/type/id 的 id, 如果是多列主键会用 "-" 拼接
      increment-column: id       # *** 必须设定. 表示用来做数据增量操作时用, 一般使用自增 id 或 time(更新时间戳)

      # 6.0 开始, type 默认是 _doc, Elasticsearch 中的 index 直接对应数据库表名
      index: product             # 可以不设定. 表示 Elasticsearch 中 /index/type/id 的 index, 不设定将会从数据库表名生成(t_some_one ==> some-one), 6.0 开始 index name 必须是小写
      type: doc                  # 可以不设定. 默认是 _doc
      scheme: false              # 可以不设定. 是否在启动时基于 数据库表结构 生成 Elasticsearch 的 scheme, 默认是 true
      sql: select * from t_table # 可以不设定. 自定义的 sql 语句(不要用 ORDER BY 和 LIMIT, 会基于 increment-column 自动添加), 不设定将会自动从数据库表拼装
      limit: 2000                # 可以不设定. 一次从数据库获取的条数, 默认是 1000

      key-column: [id]           # 可以不设定. 用来生成 index 的 id 列, 不设置将会自动从表中获取, 当表中有主键又有多列唯一约束, 想用唯一约束来做 index 的 id 时可以使用此配置
      id-prefix:                 # 可以不设定. 当想在 index 的 id 上加前缀时使用
      id-suffix:                 # 可以不设定. 当想在 index 的 id 上加后缀时使用

      mapping:                   # 可以不设定. 默认将会从表字段生成(c_some_type ==> someType), 只设置特殊情况即可
        c_type: type               # table column(如果有别名则使用别名) : elasticsearch field
      ignore-column: [c1, c2]    # 可以不设定. 上面的 sql 中不想写入索引的字段(如果字段有别名则用别名)

      big-count-to-sql: 2000     # 可以不设定. sql 中 limit start,1000 中的 start 超出这个值将会进行优化成 inner join 语句, 默认是 2000
      primary-key: id            # 可以不设定. 主键名, 当表数据很多时使用  LIMIT 1000万,1000  效率会很慢, 这个字段会优化 sql 语句, 默认是 id
      # 原来的 sql 是: SELECT ... FROM t_product WHERE time > '2010-01-01 00:00:01' LIMIT 1000万,1000
      # 优化的 sql 是: SELECT ... FROM t_product c inner join (SELECT id FROM t_product WHERE time > '2010-01-01 00:00:01' LIMIT 1000万,1000) t on t.id = c.id
      # 原先的 sql 中, limit 是最后才执行, 执行时是先通过索引找到 id, 再去存数据的物理块取相关的记录, 然后在结果集里偏移 1000万 再取 1000 条, 所以效率好不了
      # 优化的 sql 中, 索引中存的是索引列和主键, 所以括号中的查询哪怕偏移 1000万 也全都是在索引里操作(覆盖索引), 然后再用 id 联表取数据, 所以这样是很快的
#+END_SRC

cron 的说明如下
#+BEGIN_EXAMPLE
.------------------- second (0 - 59)   if (0/10) then (0, 10, 20, 30, 40, 50) run
.  .---------------- minute (0 - 59)
.  .  .------------- hour (0 - 23)
.  .  .  .---------- day of month (1 - 31)
.  .  .  .  .------- month (1 - 12)   OR jan,feb,mar,apr,may,jun,jul,aug,sep,oct,nov,dec
.  .  .  .  .  .---- day of week (0 - 6) (Sunday=0 or 7) OR sun,mon,tue,wed,thu,fri,sat
.  .  .  .  .  .
?  *  *  *  *  *

如  0/5 * * * * *  表示每 5 秒运行一次
#+END_EXAMPLE


** 运行
#+BEGIN_SRC conf
git clone https://github.com/liuanxin/mysql2es.git
cd mysql2es

更改 application-prod.yml 成你自己的配置
mvn clean package -DskipTests
nohup java -jar -Dspring.profiles.active=prod target/mysql2es.jar >/dev/null 2>&1 &

或者

添加你的配置文件到任意地方: ~/application.yml
mvn clean package -DskipTests
nohup java -jar -Dspring.config.location=~/application.yml target/mysql2es.jar >/dev/null 2>&1 &


日志在 ~/logs/mysql2es.log 中
#+END_SRC


** 说明

项目在运行时会基于表字段来生成对应 index 的 scheme(如果不想自动生成请先在 Elasticsearch 中建好并在 index 的配置中设置 *scheme* 为 false),
而后会基于定时规则来同步数据, 同步时基于 sql 拼接增量字段来获取分页数据并批量写入 Elasticsearch 直到没有数据为止,
下次同步时会用到(如果到了下次运行时间但上次还没有运行结束将会顺延).

PS:  \\
如果数据量很大, 第一次同步会耗费一点时间. 调整 *limit* 的值可以提升同步速度.
临时文件会每个索引生成一个, 比如有 ~product~ 和 ~order~ 这样两个索引, 会生成两个临时文件 ~/tmp/product~ 和 ~/tmp/order~,
windows 在 ~C:\Users\current_user\AppData\Local\Temp~ 目录下, 可以在运行的命令行加上 ~-Djava.io.tmpdir=/path/to/tmpdir~ 来修改,
如果将索引对应的临时文件手动删除, 下一次同步时此索引将会进行全量操作.
